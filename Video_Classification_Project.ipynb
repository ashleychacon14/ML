{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Video Classification Project\n",
        "By Ashley Chacon"
      ],
      "metadata": {
        "id": "EoExEoq4A3Jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Google Collab is inspired by a [Video CNN Model YouTube tutorial by Data Magic](https://www.youtube.com/watch?v=UHdrxHPRBng&ab_channel=DataMagic%28bySunnyKusawa%29). The dataset used for training and testing is the [FER-2013 dataset](https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer) found on [Kaggle.com](https://kaggle.com). This system creates an emotion classifer model using a `tf.keras.Sequential` model and a 2D Convolutional Layer."
      ],
      "metadata": {
        "id": "msRtgmANBOR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import TensorFlow and other libraries"
      ],
      "metadata": {
        "id": "xacdbLxAFhxX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZgQ8gpDBDI1W"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import PIL\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import model_from_json\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessing my own Google Drive to run project. Download dataset [here](https://www.kaggle.com/datasets/ananthu017/emotion-detection-fer) and upload to this Collab to run project on your own machine and set `reextract_dataset` to `True` in order for the code to properly run."
      ],
      "metadata": {
        "id": "sMfiOPV-FJ6m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YQy_Oh-Hm9EU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14885d8-a70f-4134-8d6a-3a4f2eb0ddde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/VideoEmotionRecog\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Research/VideoEmotionRecog/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, before creating a model to classify emotion in video. We must first train a model to classify emotion in images. We use the uploaded dataset to do so."
      ],
      "metadata": {
        "id": "M1Wp41j3Mxi5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PwrORn9VFVMz"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import zipfile\n",
        "\n",
        "reextract_dataset = False\n",
        "\n",
        "if reextract_dataset:\n",
        "  with zipfile.ZipFile('images-classification.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rescale data and assign training and validation variables"
      ],
      "metadata": {
        "id": "6JGaQACsLEb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network; in general you should seek to make your input values small. Now, split your data in its designated space, train or validation."
      ],
      "metadata": {
        "id": "Rb8ZUElcLOyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MUixRFiuDQOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcdd0ff5-9858-42d1-d455-af79bb6a16dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3658 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ],
      "source": [
        "# Initialize image data generator with rescaling\n",
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "validation_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Preprocess all test images\n",
        "train_generator = train_data_gen.flow_from_directory(\n",
        "        './train',\n",
        "        target_size=(48, 48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Preprocess all train images\n",
        "validation_generator = validation_data_gen.flow_from_directory(\n",
        "        './test',\n",
        "        target_size=(48, 48),\n",
        "        batch_size=64,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implemented Sequential model with 2D Convolutional Layers"
      ],
      "metadata": {
        "id": "V6cQDgd5La6W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following neural network was provided by [Data Magic's tutorial]((https://www.youtube.com/watch?v=UHdrxHPRBng&ab_channel=DataMagic%28bySunnyKusawa%29)), which was explained step by step."
      ],
      "metadata": {
        "id": "QihQFroYLz62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "MFOJ-aR3DwxW"
      },
      "outputs": [],
      "source": [
        "emotion_model = Sequential()\n",
        "\n",
        "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
        "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Flatten())\n",
        "emotion_model.add(Dense(1024, activation='relu'))\n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "emotion_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train your model for 50 epochs, then save the trained image model into a JSON file, and the model's weights in a .h5 file."
      ],
      "metadata": {
        "id": "VvmFvbawMibq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "coe8mMUXESCR"
      },
      "outputs": [],
      "source": [
        "retrain_model = False\n",
        "\n",
        "if retrain_model:\n",
        "  emotion_model_info = emotion_model.fit_generator(\n",
        "          train_generator,\n",
        "          steps_per_epoch=28709 // 64,\n",
        "          epochs=50,\n",
        "          validation_data=validation_generator,\n",
        "          validation_steps=7178 // 64)\n",
        "\n",
        "  # save model structure in jason file\n",
        "  model_json = emotion_model.to_json()\n",
        "  with open(\"emotion_model.json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "\n",
        "  # save trained model weight in .h5 file\n",
        "  emotion_model.save_weights('emotion_model.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, create `emotion_dict` to select what emotions our video model can choose from. Load your trained image model and its weights."
      ],
      "metadata": {
        "id": "0Fcm4gCsNBm8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "FvNydIQMEWFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd71cb76-8b19-4d8f-bdd1-8078cf9ef82a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded model from disk\n"
          ]
        }
      ],
      "source": [
        "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('./emotion_model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "emotion_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "emotion_model.load_weights(\"./emotion_model.h5\")\n",
        "print(\"Loaded model from disk\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you have the option to use your own webcam for the model to predict the expression on your face! Instead, we use a [sample video](https://www.videezy.com/free-video/mp4-videos). "
      ],
      "metadata": {
        "id": "sJY258m3Ndha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "zcn5OX9bEZjx"
      },
      "outputs": [],
      "source": [
        "# start the webcam feed\n",
        "#cap = cv2.VideoCapture(0)\n",
        "\n",
        "# pass here your video path\n",
        "# you may download one from here : https://www.pexels.com/video/three-girls-laughing-5273028/\n",
        "# the following video is being tested in this program : https://www.videezy.com/people/8445-dark-haired-girl-in-disbelief-1\n",
        "cap = cv2.VideoCapture(\"./Sample-Vid02.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using a Cascade Classifier to find face and predict emotion it displays"
      ],
      "metadata": {
        "id": "4dSBDZdOPSi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following code gray scales the timestamp of the video, since the image model is trained on gray scale images. The model then uses a Cascade Classifier to find the face in the video, then uses the image model to predict the emotion the person is displaying."
      ],
      "metadata": {
        "id": "BmVVpzEsPkN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_px_offset = 50\n",
        "rect_color = (0, 0, 255)  # BGR\n",
        "rect_thick = 2\n",
        "\n",
        "rect_px_offset = 10\n",
        "\n",
        "adjusted_size = (48,48)\n",
        "\n",
        "x_coord_adjust = 5\n",
        "y_coord_adjust = 10\n",
        "font_scale = 1\n",
        "text_color = (255, 255, 0) # BGR\n",
        "text_thick = 2"
      ],
      "metadata": {
        "id": "kO8U9eR08qKs"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download this project to your machine for results. Displaying results results in a large file size, and I am not able to upload the project to GitHub."
      ],
      "metadata": {
        "id": "Km7iQo2REflr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEjB9Cu9Eeqn"
      },
      "outputs": [],
      "source": [
        "for _ in range(90):\n",
        "    # Find haar cascade to draw bounding box around face\n",
        "    ret, frame = cap.read()\n",
        "    frame = cv2.resize(frame, (1280, 720))\n",
        "    if not ret:\n",
        "        break\n",
        "    face_detector = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # detect faces available on camera\n",
        "    faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "    # take each face available on the camera and Preprocess it\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y-label_px_offset), (x+w, y+h+rect_px_offset), rect_color, rect_thick)\n",
        "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
        "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, adjusted_size), -1), 0)\n",
        "\n",
        "        # predict the emotions\n",
        "        emotion_prediction = emotion_model.predict(cropped_img)\n",
        "        maxindex = int(np.argmax(emotion_prediction))\n",
        "        cv2.putText(frame, emotion_dict[maxindex], (x+x_coord_adjust, y-y_coord_adjust), \n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, text_thick, cv2.LINE_AA)\n",
        "\n",
        "    cv2.waitKey(0)\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "cap.release()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Video Classification Project",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}